{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링(웹 크롤링)\n",
    "크롤링이란\n",
    "- 인터넷 상의 웹 페이지 데이터를 자동으로 수집하는 과정.\n",
    "- 웹 크롤링은 일반적으로 웹 스크래핑과 연관되며, 둘은 종종 혼용되지만 조금 다른 개념. 웹 크롤링은 웹 페이지를 탐색하고 데이터를 수집하는 반면, 웹 스크래핑은 그 페이지에서 특정 정보를 추출하는 데 중점을 둔다.\n",
    "- 크롤링은 스크래핑을 포함할 수 있다. 크롤링 과정에서 각 페이지를 방문할 때, 스크래핑을 통해 필요한 데이터를 추출할 수 있다.\n",
    "\n",
    "웹 크롤링에 사용되는 도구\n",
    "- BeautifulSoup: Python 라이브러리로, HTML 및 XML 문서를 구문 분석하고 데이터를 추출하는 데 사용.\n",
    "\n",
    "- Scrapy: 웹 크롤링을 위한 Python 프레임워크로, 효율적이고 확장성이 높은 크롤러를 쉽게 만들 수 있다.\n",
    "\n",
    "- Selenium: 웹 브라우저 자동화 도구로, JavaScript가 동적으로 로드되는 페이지를 크롤링할 때 유용.\n",
    "\n",
    "- Requests: Python의 HTTP 라이브러리로, 웹 페이지 요청을 쉽게 할 수 있다.\n",
    "\n",
    "웹 크롤링의 기본 과정\n",
    "- 크롤러 설정: 크롤러는 특정 웹 페이지를 시작점으로 설정. 이를 '시드(seed)'라고 부르며, 크롤러는 이 시드 URL에서 시작해 다른 페이지로 이동.\n",
    "\n",
    "- 페이지 요청: 크롤러는 HTTP 요청을 보내 웹 페이지를 요청. 이 과정에서 크롤러는 브라우저처럼 행동하여 웹 서버에서 페이지를 가져온다.\n",
    "\n",
    "- 데이터 추출: 웹 페이지가 응답되면, 크롤러는 페이지의 HTML을 분석하고 필요한 데이터를 추출. 이 과정에는 BeautifulSoup, Selenium 같은 라이브러리가 사용될 수 있다.\n",
    "\n",
    "- 링크 추출 및 큐잉: 크롤러는 현재 페이지에서 다른 페이지로 연결되는 링크를 추출하고, 이 링크들을 큐(queue)에 추가하여 다음 크롤링 대상으로 삼는다.\n",
    "\n",
    "- 반복: 이 과정은 정해진 규칙이나 종료 조건이 충족될 때까지 반복. 예를 들어, 특정 수의 페이지를 크롤링하거나, 주어진 도메인 내에서만 크롤링하도록 설정할 수 있다.\n",
    "\n",
    "웹 크롤링의 주의사항\n",
    "\n",
    "- 로봇 배제 표준(robots.txt): 많은 웹사이트는 robots.txt 파일을 통해 크롤러가 접근 가능한 부분과 접근을 제한하는 부분을 명시. 크롤러는 이 규칙을 준수해야 한다.\n",
    "\n",
    "- 저작권 및 법적 이슈: 모든 웹사이트의 콘텐츠는 저작권의 보호를 받는다. 따라서 크롤링을 통해 수집한 데이터를 어떻게 사용할지에 대한 법적 문제를 주의해야 한다.\n",
    "\n",
    "- 서버 부하: 지나친 크롤링은 웹 서버에 부하를 줄 수 있다. 크롤링 시에는 서버의 부담을 줄이기 위해 요청 간의 딜레이를 설정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup\n",
    "- BeautifulSoup은 HTML이나 XML 문서를 파싱하고, 파싱한 데이터에서 원하는 요소를 검색하고 추출하는 데 매우 유용한 도구입니다. \n",
    "- BeautifulSoup에서 객체를 찾는 주요 방법에는 find, find_all, select_one, select, find_parents, find_parent, find_next_sibling, find_previous_sibling, find_parents, find_parent 등이 있습니다.\n",
    "\n",
    "검색 방식\n",
    "- find, find_all: 태그 이름과 속성을 사용하여 요소를 검색합니다.\n",
    "- select_one, select: CSS 선택자를 사용하여 요소를 검색합니다. id, class\n",
    "\n",
    "반환 결과:\n",
    "- find: 첫 번째로 일치하는 요소를 반환합니다.\n",
    "- find_all: 모든 일치하는 요소를 리스트로 반환합니다.\n",
    "- select_one: 첫 번째로 일치하는 요소를 반환합니다.\n",
    "- select: 모든 일치하는 요소를 리스트로 반환합니다.\n",
    "\n",
    "표현력:\n",
    "- select_one, select: 더 복잡하고 정교한 선택 조건을 지정할 수 있습니다. 예를 들어, CSS 선택자 문법을 사용하여 클래스, ID, 속성 등을 조합한 검색이 가능합니다.\n",
    "- find, find_all: 단순한 태그 이름과 속성 조건에 기반한 검색이 주로 사용됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html.parser vs. lxml\n",
    "- 파이썬에서 HTML 및 XML 문서를 파싱(parsing)하는 라이브러리\n",
    "- html.parser는 HTML 문서를 파싱하는 데에 적합한 파서. 파이썬의 기본 라이브러리로 제공되며 파이썬 내부적으로 구현되어 있으며, 외부 종속성이 없으므로 파이썬과 함께 설치되는 패키지만 사용할 수 있습니다.\n",
    "- lxml은 C 언어로 작성된 파이썬 외부 라이브러리로서 HTML 및 XML 문서를 파싱하는 데에 적합하며, 파서 성능이 매우 우수합니다.\n",
    "- HTML 문서를 파싱하는 경우에는 html.parser를 사용하는 것이 간단하고 편리하며, 대부분의 경우에는 충분한 성능을 제공합니다. 그러나 대용량의 XML 문서나 매우 복잡한 HTML 문서를 파싱해야 하는 경우에는 lxml을 사용하는 것이 더 효율적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '<html><body><h1>Title</h1><p class=\"content\">First paragraph.</p><p class=\"content\">Second paragraph.</p></body></html>'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1_0619. 다음 사항을 수행하세요.\n",
    "- 첫 번째로 매칭되는 'p' 태그 찾기\n",
    "- 모든 'p' 태그 찾기\n",
    "- 클래스가 'content'인 첫 번째 'p' 태그 찾기\n",
    "- 클래스가 'content'인 모든 'p' 태그 찾기\n",
    "- 특정 'p' 태그의 모든 부모 태그 찾기\n",
    "- 특정 'p' 태그의 첫 번째 부모 태그 찾기\n",
    "- 특정 'p' 태그의 다음 형제 태그 찾기\n",
    "- 특정 'p' 태그의 이전 형제 태그 찾기\n",
    "- 특정 'p' 태그 다음에 위치한 모든 태그나 문자열 찾기\n",
    "- 특정 'p' 태그 이전에 위치한 모든 태그나 문자열 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫 번째로 매칭되는 'p' 태그 찾기\n",
    "print('1. ',end = '')\n",
    "target_p = soup.find('p')\n",
    "print(target_p)\n",
    "\n",
    "#모든 'p' 태그 찾기\n",
    "print('2. ',end = '')\n",
    "target_p = soup.find_all('p')\n",
    "for i in target_p:\n",
    "    print(i)\n",
    "\n",
    "#- 클래스가 'content'인 첫 번째 'p' 태그 찾기\n",
    "print('3. ',end = '')\n",
    "target_p = soup.select_one('p.content')\n",
    "print(target_p)\n",
    "\n",
    "#- 클래스가 'content'인 모든 'p' 태그 찾기\n",
    "print('4. ',end = '')\n",
    "target_p = soup.select('p',class_='content')\n",
    "for i in target_p:\n",
    "    print(i)\n",
    "\n",
    "#- 특정 'p' 태그의 모든 부모 태그 찾기\n",
    "# print('5. ',end = '')\n",
    "# target_p = soup.select('p')\n",
    "# for i in target_p:\n",
    "    # print(i.parent)\n",
    "\n",
    "#- 특정 'p' 태그의 모든 부모 태그 찾기\n",
    "# [document] -> <html> -> <body> -> <p>: 최상위 요소 document에서 시작하여 p 태그까지의 모든 부모 요소를 출력\n",
    "print('5. ',end = '')\n",
    "target_p = soup.find('p', class_='content')\n",
    "target = target_p.find_parents()\n",
    "print(target)\n",
    "\n",
    "#- 특정 'p' 태그의 첫 번째 부모 태그 찾기\n",
    "print('6. ',end = '')\n",
    "# target_p = soup.select_one('p')\n",
    "# print(target_p.parent)\n",
    "target = target_p.find_parent()\n",
    "print(target)\n",
    "#- 특정 'p' 태그의 다음 형제 태그 찾기\n",
    "print('7. ',end = '')\n",
    "# target_p = soup.select_one('p')\n",
    "# print(target_p.next_sibling)\n",
    "target = target_p.find_next_sibling()\n",
    "print(target)\n",
    "#- 특정 'p' 태그의 이전 형제 태그 찾기\n",
    "print('8. ',end = '')\n",
    "# target_p = soup.select_one('p')\n",
    "# print(target_p.previous_sibling)\n",
    "target = target_p.find_previous_sibling()\n",
    "print(target)\n",
    "#- 특정 'p' 태그 다음에 위치한 모든 태그나 문자열 찾기\n",
    "print('9. ',end = '')\n",
    "# target_p = soup.select_one('p')\n",
    "# aaa = target_p.next_siblings\n",
    "# for i in aaa:\n",
    "#     print(i)\n",
    "target = target_p.find_next()\n",
    "print(target)\n",
    "\n",
    "#- 특정 'p' 태그 이전에 위치한 모든 태그나 문자열 찾기 \n",
    "print('10. ',end = '')\n",
    "# target_p = soup.select_one('p')\n",
    "# aaa = target_p.previous_siblings\n",
    "# for i in aaa:\n",
    "#     print(i)\n",
    "target = target_p.find_previous()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 시 헤더를 포함하면 성공적으로 데이터를 가능성을 높여준다\n",
    "\n",
    "headers : HTTP 요청에 포함되는 메타데이터\n",
    "- User-Agent: 클라이언트 애플리케이션(브라우저 등)을 나타냅니다.\n",
    "- Accept: 서버가 어떤 콘텐츠 타입을 반환해야 하는지 지정합니다.\n",
    "- Accept-Language: 클라이언트가 선호하는 언어를 지정합니다.\n",
    "- Referer: 요청이 발생한 이전 페이지의 URL을 지정합니다.\n",
    "- Host: 요청을 보내는 서버의 호스트 이름을 지정합니다.\n",
    "- Connection: 서버와 클라이언트 간의 연결 유형을 지정합니다.\n",
    "\n",
    "```\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': 'http://example.com',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://news.naver.com'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "for i in range(5):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response.content:\n",
    "- response.content는 서버에서 반환된 응답을 바이트(byte) 문자열로 제공합니다.\n",
    "- 주로 이미지, 파일 다운로드와 같은 바이너리 데이터를 다룰 때 사용됩니다.\n",
    "- 인코딩과 상관없이 원본 그대로의 데이터를 가져오기 때문에, HTML 파싱을 할 때는 별도로 인코딩을 지정하지 않으면 기본 인코딩을 사용합니다.\n",
    "\n",
    "response.text:\n",
    "- response.text는 서버에서 반환된 응답을 유니코드 문자열로 제공합니다.\n",
    "- requests 라이브러리는 response.text를 반환할 때, response.encoding에 지정된 인코딩을 사용하여 바이트 데이터를 유니코드 문자열로 디코딩합니다.\n",
    "- 일반적인 텍스트 데이터, HTML, JSON 등의 처리를 할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select_one('title')\n",
    "target.text\n",
    "target.get_text()\n",
    "target.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2_0619. ID를 이용해서 '네이버 뉴스' 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select_one('#browserTitleArea')\n",
    "target.text\n",
    "target.get_text()\n",
    "target.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.find(id='browserTitleArea')\n",
    "target.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.find_all(class_=\"Nitem_link_menu\")\n",
    "\n",
    "for idx, i in enumerate(target):\n",
    "    print(f\"{idx +1}: {i.get_text().strip()}\")\n",
    "\n",
    "#for i in target:\n",
    "#    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3_0619. soup.find_all(class_='Nitem_link_menu') 대신에 select를 이용하여 동일한 결과를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select('span.Nitem_link_menu')\n",
    "\n",
    "for idx, i in enumerate(target):\n",
    "    print(f\"{idx +1}: {i.get_text().strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task4_0619. select_one을 이용해서 'https://news.naver.com'에서 \"뉴스\"를 출력하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select_one('span.Nicon_service')\n",
    "print(target.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task5_0619.'https://news.naver.com'에서 아래 예시와 같이 뉴스 기사 제목을 모두 출력하세요. \n",
    "\n",
    "예시: 1: [속보] '훈련병 사망' 얼차려 지시 중대장·부중대장 피의자 신분 첫 소환조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select('div.cjs_t')\n",
    "\n",
    "for idx, i in enumerate(target):\n",
    "    print(f\"{idx +1}: {i.get_text().strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string vs. get_text() vs. text\n",
    "- BeautifulSoup 객체에서 text 속성, get_text() 메서드, string 속성은 모두 HTML 또는 XML 문서에서 텍스트 데이터를 추출하는 데 사용\n",
    "- text 속성은 해당 태그에서 모든 텍스트 데이터를 가져오며,\n",
    "- get_text() 메서드도 동일한 결과를 반환합니다.\n",
    "- string 속성은 해당 태그에서 첫 번째로 발견된 문자열 데이터만 가져옵니다.\n",
    "\n",
    "| Method      | 특징                                                                               | 사용 사례                                      |\n",
    "|-------------|----------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| `string`    | 자식 텍스트 노드가 하나인 경우에만 해당 텍스트 반환, 여러 노드가 있으면 `None` 반환                | 단일 텍스트 노드만 있는 요소의 텍스트를 추출할 때      |\n",
    "| `get_text()`| 요소 및 모든 하위 요소의 텍스트를 모두 포함하여 문자열로 반환, `separator` 및 `strip` 옵션 사용 가능 | 요소 내 모든 텍스트를 추출하고, 구분자 및 공백 처리가 필요한 경우 |\n",
    "| `text`      | 요소 및 모든 하위 요소의 텍스트를 모두 포함하여 문자열로 반환, 추가 매개변수 없음                  | 요소 내 모든 텍스트를 단순하게 추출할 때                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string\n",
    "- 정의: string 속성은 BeautifulSoup 객체의 자식 노드 중 하나의 텍스트 노드만 반환. 해당 요소의 직접적인 텍스트 콘텐츠가 하나일 때만 유용.\n",
    "- 특징:\n",
    "요소가 하나의 자식 텍스트 노드를 가지고 있는 경우에만 그 텍스트를 반환.\n",
    "여러 자식 요소가 있거나 텍스트 노드가 여러 개 있는 경우 None을 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_text()\n",
    "- 정의: get_text() 메서드는 요소 및 모든 하위 요소의 텍스트를 모두 추출하여 하나의 문자열로 반환.\n",
    "- 특징:\n",
    "요소 내의 모든 텍스트를 포함하여 반환.\n",
    "기본적으로 하위 요소 사이에 공백을 추가하지만, separator 매개변수를 사용하여 구분자를 지정할 수 있다.\n",
    "strip=True 옵션을 사용하여 앞뒤 공백을 제거할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<div class=\"example\">\n",
    "    Single text node\n",
    "</div>\n",
    "<div class=\"example\">\n",
    "    <p>Multiple</p> text <span>nodes</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_text_node = soup.find('div', class_='example').string\n",
    "multiple = soup.find_all('div',class_='example')\n",
    "\n",
    "multiple_text_nodes = soup.select('div', calss_='example')[1].text\n",
    "\n",
    "print(single_text_node)\n",
    "print(multiple,'\\n')\n",
    "print(multiple_text_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 'get_text()' 메서드는 공백을 구분자로 사용합니다.\n",
    "# strip=True 옵션으 사용하여 앞뒤 공백을 제거하고 텍스트를 추출\n",
    "single_text_node = soup.find('div', class_='example').get_text(strip=True)\n",
    "multiple_text_nodes = soup.find_all('div', class_='example')[1].get_text(strip=True)\n",
    "\n",
    "print('Default separator:')\n",
    "print(single_text_node)\n",
    "print(multiple_text_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'separator' 매개변수를 사용하여 구분자를 지정합니다.\n",
    "single_next_node_with_separator = soup.find('div', class_='example').get_text(strip=True)\n",
    "multiple_text_nodes_with_separator = soup.find_all('div', class_='example')[1].get_text(separator=' | ',strip=True)\n",
    "\n",
    "print(single_next_node_with_separator)\n",
    "print(multiple_text_nodes_with_separator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text\n",
    "- 정의: text 속성은 요소 및 모든 하위 요소의 텍스트를 모두 포함하는 문자열을 반환. get_text() 메서드와 동일한 기능을 한다.\n",
    "- 특징:\n",
    "get_text()와 거의 동일한 결과를 제공.\n",
    "get_text()와 다르게 추가 매개변수(separator, strip)를 사용할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_text_node = soup.find('div', class_='example').text\n",
    "multiple = soup.find_all('div',class_='example')\n",
    "\n",
    "multiple_text_nodes = soup.select('div', calss_='example')[1].text\n",
    "\n",
    "print(single_text_node)\n",
    "print(multiple,'\\n')\n",
    "print(multiple_text_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규표현식을 이용한 크롤링 방법 \n",
    "\n",
    "HTML 가져오기:\n",
    "- requests 라이브러리를 사용하여 웹 페이지의 HTML을 가져옵니다.\n",
    "HTML 파싱:\n",
    "- BeautifulSoup을 사용하여 HTML 문서를 파싱합니다.\n",
    "정규표현식 사용:\n",
    "- re 모듈을 사용하여 특정 패턴을 가진 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = \"\"\"\n",
    "<ul>\n",
    "    <li>Email: example@example.com</li>\n",
    "    <li>Contact: contact@sample.org</li>\n",
    "</ul>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#이메일 주소 추출\n",
    "email_pattern = r'[a-zA-Z0-9_,+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "emails = re.findall(email_pattern,html)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "<body>\n",
    "<div>\n",
    "Hello,world!\n",
    "</div>\n",
    "<div>\n",
    "<p>\n",
    "Hello,<b>world!</b>\n",
    "</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "bs = soup.body.text\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texts = re.findall('[^\\s]+',bs)\n",
    "for t in texts:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('Hello,world!',bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs.replace('\\n',''))\n",
    "print(re.sub('\\n','',bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub('\\n\\n',' ',bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = ''\n",
    "for t in texts:\n",
    "    li += t\n",
    "    li += ' '\n",
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "<head>\n",
    "<title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "<div>\n",
    "Hello, world! 123\n",
    "</div>\n",
    "<div>\n",
    "<p>\n",
    "456 Hello, <b>789 world!</b> Visit us at <a href=\"http://example.com\">example.com</a>\n",
    "</p>\n",
    "</div>\n",
    "<footer>\n",
    "Contact us at <a href=\"mailto:info@example.com\">info@example.com</a>\n",
    "</footer>\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 주어진 html에서 아래 사항을 수행하세요.\n",
    "- 모든 단어 추출\n",
    "- 이메일 주소 추출\n",
    "- url 추출\n",
    "- 숫자 추출\n",
    "- HTML 태그 내 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 단어 추출\n",
    "target = re.findall('[a-zA-Z]+', html)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 단어 추출\n",
    "# \\b 단어 경계, \\w 단어 문자\n",
    "words_list = ''\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "words = re.findall(r'\\b\\w+\\b', soup.text)\n",
    "for word in words:\n",
    "    words_list += word\n",
    "    words_list += ', '\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이메일 주소 추출\n",
    "target = re.findall('[a-zA-Z0-9_,+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', html)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이메일 주소 추출\n",
    "email_pattern = '[a-zA-Z0-9_,+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "emails = re.findall(email_pattern, html)\n",
    "for email in emails:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 추출\n",
    "target = re.findall('https?://[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', html)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 추출\n",
    "url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>]+'\n",
    "urls = re.findall(url_pattern,html)\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 추출\n",
    "target = re.findall('[0-9]',html)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 추출\n",
    "num_list = ''\n",
    "numbers = re.findall(r'\\b\\d+\\b', html)\n",
    "for number in numbers:\n",
    "    num_list += number\n",
    "    num_list += ' '\n",
    "print(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그 내 텍스트 추출\n",
    "target = re.findall('^>\\s?[a-zA-Z0-9\\s]+<+/+',html)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그 내 텍스트 추출\n",
    "tag_texts = re.findall(r'>([^<]+)<', html)\n",
    "text_list = ' '.join(text.strip() for text in tag_texts)\n",
    "text_list = re.sub(r'\\s+',' ', text_list)\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = \"\"\"\n",
    "<ul>\n",
    "  <li><a href=\"hoge.html\">hoge</li>\n",
    "  <li><a href=\"https://example.com/fuga\">fuga*</li>\n",
    "  <li><a href=\"https://example.com/foo\">foo*</li>\n",
    "  <li><a href=\"http://example.com/aaa\">aaa</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# 정규 표현식으로 href에서 https인 것 추출하기\n",
    "li = soup.find_all(href=re.compile(r\"^https://\"))\n",
    "print(li)\n",
    "for e in li: print(e.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://news.naver.com/section/100'\n",
    "\n",
    "html = requests.get(url)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib + bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as rq\n",
    "\n",
    "url = 'https://news.naver.com/section/100'\n",
    "\n",
    "html = rq.urlopen(url)\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = bs.find('p')\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = bs.find_all('p')\n",
    "for t in text10:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find('p',class_='sa_head_layer_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = bs.find('p').string\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs.find('title'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs.find('title').string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = bs.find('p').get_text()\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### request + bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코딩 에러를 해결\n",
    "- chardet는 \"Universal Character Encoding Detector\"로, 다양한 인코딩을 감지할 수 있는 파이썬 패키지\n",
    "- chardet.detect(response.content)['encoding']은 response.content의 인코딩 방식을 자동으로 감지하여 반환하며\n",
    " 이 값을 encoding 변수에 저장한 후,<br> response.content를 이 encoding 방식으로 디코딩하여 html 변수에 저장하고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import chardet\n",
    "\n",
    "# HTTP 요청에서 사용될 헤더 정보를 설정합니다\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "\n",
    "# 웹사이트에서 스크래핑할 URL을 지정합니다.\n",
    "url = 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=100'\n",
    "\n",
    "# 지정환 url에 HTTP GET 요청을 전달\n",
    "response = requests.get(url, headers= headers)\n",
    "\n",
    "# HTTP 요청이 성공적으로 전달되었다면, 웹사이트의 HTML 코드를 출력합니다\n",
    "if response.status_code==200:\n",
    "    encoding = chardet.detect(response.content)['encoding']\n",
    "    # print(response.content.decode(encoding)) # 웹사이트의 HTML 코드를 포함하는 이진(binary) 데이터를 반환\n",
    "    print(response.text) #웹사이트의 HTML 코드를 문자열 형태로 반환\n",
    "else:\n",
    "    print('HTTP request failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한글만 출력\n",
    "import re\n",
    "texts = response.text\n",
    "result = re.findall('[가-힇]+',texts)\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "\n",
    "url = \"https://serieson.naver.com/v3/movie/ranking/realtime\"\n",
    "response = requests.get(url, headers= headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "target = soup.select_one('span.Title_title__s9o0D')\n",
    "\n",
    "\n",
    "target.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select('span.Title_title__s9o0D')\n",
    "for idx, i in enumerate(target):\n",
    "    print(f'{idx+1}, ',i.text)\n",
    "    if idx>8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = soup.select('span.Title_title__s9o0D')\n",
    "for idx, i in enumerate(target[:10]):\n",
    "    print(f'{idx+1}, ',i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1_0620. 네이버 영화 순위 사이트에서 영화제목, 가격, 타입(구매 or 대여) 정보를 가져와서 TITLE, PRICE, TYPE 3개의 컬럼과 100개의 데이터포인트로 구성된 데이터프레임을 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "\n",
    "url = \"https://serieson.naver.com/v3/movie/ranking/realtime\"\n",
    "response = requests.get(url, headers= headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인사이드 아웃(패키지상품 : 더빙판 + 부가영상 추가증정)\n",
      "구매\n",
      "7,150캐시\n"
     ]
    }
   ],
   "source": [
    "target = soup.select_one('span.Title_title__s9o0D')\n",
    "print(target.text)\n",
    "type = target.next_sibling.select_one('span.Price_text__pRk_f')\n",
    "print(type.text)\n",
    "price = target.next_sibling.select_one('span.Price_price__GqXqo')\n",
    "print(price.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사이드 아웃(패키지상품 : 더빙판 + 부가영상 추가증정)</td>\n",
       "      <td>7,150캐시</td>\n",
       "      <td>구매</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>파묘</td>\n",
       "      <td>7,000캐시</td>\n",
       "      <td>대여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>명탐정 코난 VS 괴도 키드</td>\n",
       "      <td>11,000캐시</td>\n",
       "      <td>대여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>설계자</td>\n",
       "      <td>11,000캐시</td>\n",
       "      <td>대여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>랜드 오브 배드</td>\n",
       "      <td>5,000캐시</td>\n",
       "      <td>대여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>컨저링</td>\n",
       "      <td>1,300캐시</td>\n",
       "      <td>대여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>아티스트 봉만대_무삭제판</td>\n",
       "      <td>6,500캐시</td>\n",
       "      <td>구매</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>닥터(무삭제 감독판)</td>\n",
       "      <td>1,000캐시</td>\n",
       "      <td>구매</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>크루즈 패밀리(패키지: 자막판+더빙판)</td>\n",
       "      <td>5,000캐시</td>\n",
       "      <td>구매</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>제로 다크 서티</td>\n",
       "      <td>1,000캐시</td>\n",
       "      <td>구매</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                TITLE     PRICE TYPE\n",
       "1    인사이드 아웃(패키지상품 : 더빙판 + 부가영상 추가증정)   7,150캐시   구매\n",
       "2                                  파묘   7,000캐시   대여\n",
       "3                     명탐정 코난 VS 괴도 키드  11,000캐시   대여\n",
       "4                                 설계자  11,000캐시   대여\n",
       "5                            랜드 오브 배드   5,000캐시   대여\n",
       "..                                ...       ...  ...\n",
       "96                                컨저링   1,300캐시   대여\n",
       "97                      아티스트 봉만대_무삭제판   6,500캐시   구매\n",
       "98                        닥터(무삭제 감독판)   1,000캐시   구매\n",
       "99              크루즈 패밀리(패키지: 자막판+더빙판)   5,000캐시   구매\n",
       "100                          제로 다크 서티   1,000캐시   구매\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"TITLE\": [],\"PRICE\":[],\"TYPE\":[]})\n",
    "target = soup.select('span.Title_title__s9o0D')\n",
    "for idx, t in enumerate(target):\n",
    "    a= t.text\n",
    "    b = t.next_sibling.select_one('span.Price_text__pRk_f').text\n",
    "    c = t.next_sibling.select_one('span.Price_price__GqXqo').text\n",
    "    df.loc[idx+1] = a,c,b\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS 선택자\n",
    "- 원하는 정보만 선별하여 수집하고 싶을 때 css선택자를 활용할 수 있음\n",
    "- (CSS 선택자 설명 추가)\n",
    "- F12 >> 수집하고 싶은 부분 클릭 >> 태그 선택 >> copy Selector\n",
    "- BeautifulSoup의 select_one, select 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개발자 도구상에서 .class로 확인한 정치기사 개수는 58개인 반면에 select('.class')로 크롤링한 기사 개수는 38개인 이유\n",
    "- 많은 사이트는 JavaScript를 사용하여 콘텐츠를 동적으로 로딩하는 반면 requests + Beautifulsoup은 기본적으로 JavaScript를 실행하지 않기 때문에 일부 js로\n",
    "로딩되는 콘텐츠는 크롤링 되지 않는다.\n",
    "- 페이지가 완전히 로드되기 전에 크롤링이 시도될 경우 일부 콘텐츠가 누락될 수 있다.\n",
    "- 웹 페이지가 비동기 요청(Ajax)를 사용하여 데이터를 가져오는 경우 이 요청을 수동으로 처리하지 않는 한 해당 데이터를 가져오지 못할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://news.daum.net/politics#1'\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "response = requests.get(url, headers= headers)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "target = soup.select('.link_txt')\n",
    "n_list = []\n",
    "for i in target:\n",
    "    n_list.append(i)\n",
    "\n",
    "df=pd.DataFrame({'n_title':n_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개발자 도구 copy.selector\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://news.daum.net/politics#1'\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "\n",
    "r = rq.get(url, headers=headers)\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# 개발자 도구 copy.selector\n",
    "titles=soup.select('body > div > main > section > div > div > ul > li > div > div > strong > a')\n",
    "\n",
    "for i ,title in enumerate(titles,start =1):\n",
    "    print(f\"{i}, {title.get_text().strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "lines = soup.select('body > div > main > section > div > div > ul > li > div > div > strong > a')\n",
    "\n",
    "for i, tag in enumerate(lines, start=1):\n",
    "    text = tag.get_text().strip().replace(',','')\n",
    "    matches = re.findall('[가-힣0-9]+', text)\n",
    "    print(f\"{i}, {' '.join(matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2_0620. url = 'https://news.daum.net/politics#1'은 정치기사 1페이지인데 10페이지에 있는 기사를 모두 출력하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "n_list = []\n",
    "for i in range(1, 11):\n",
    "    url = 'https://news.daum.net/politics#' + f'{i}'\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "    response = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    target = soup.select('a.link_txt')\n",
    "    \n",
    "    for idx, i in enumerate(target):\n",
    "        n_list.append(i.text)\n",
    "        if idx ==24:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame({'n_title':n_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://news.daum.net/politics#1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m#웹 페이지 로드를 보장하기 위해 3초 쉬기\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m button \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_css_selector\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline > div\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     20\u001b[0m button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     22\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_css_selector'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "n_list = []\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://news.daum.net/politics#1')\n",
    "time.sleep(3) #웹 페이지 로드를 보장하기 위해 3초 쉬기\n",
    "button = driver.find_elements_by_css_selector('timeline > div')[2]\n",
    "button.click()\n",
    "\n",
    "time.sleep(3)\n",
    "for i in range(1, 11):\n",
    "    url = 'https://news.daum.net/politics#' + f'{i}'\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"}\n",
    "    response = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    target = soup.select('a.link_txt')\n",
    "    \n",
    "    for idx, i in enumerate(target):\n",
    "        n_list.append(i.text)\n",
    "        if idx ==24:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame({'n_title':n_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    url = 'https://news.daum.net/politics#' + f'{i}'\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<a class=\"link_txt\" href=\"https://v.daum.net/v/20240620171142273\">野, 채상병 수사외압 의혹 따진다...21일 청문회, 누가 나오나</a>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "url = 'https://news.daum.net/politics#1'\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.45.45 Safari/537.36\"}\n",
    "response = requests.get(url, headers= headers)\n",
    "\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "test = soup.select_one('a.link_txt')\n",
    "\n",
    "print(test)\n",
    "del url, headers, response, soup, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‘이재명 추가 기소’에 “모든 게 李 탓? vs “대표 아니면 벌써 기소”[중립기어]',\n",
       " '여·야, 원구성 협상 또 합의점 못 찾아… 입장차만 확인',\n",
       " '핵잠·ICBM 기술 이전 길 텄나…北-러 ‘방위능력 공동조치’ 명시',\n",
       " \"농해수위, 야당 주도 '양곡관리법·농안법' 상정…여당 불참\",\n",
       " '우상호 \"저도 \\'왕수박\\'으로 몰려…당에 \\'나 같은 역할\\' 안 보여\"',\n",
       " \"부산시의회, 부산테크노파크 원장 '2+1년' 이후 연임 집중 질의\",\n",
       " \"국힘 박상웅 국회의원, 분기별 '생활안정기금' 도입 요구\",\n",
       " '아동수당 증액법 발의 전진숙…\"가족예산 아끼지 말아야\"[파워초선]',\n",
       " '‘채상병 특검법’ 입법청문회 D-1…‘윗선’ 어디까지 출석할까',\n",
       " '방위사업청 \"英 방산업체 상대 6천9백억 원대 손해배상 소송 최종 승소\"',\n",
       " '野, 채상병 수사외압 의혹 따진다...21일 청문회, 누가 나오나',\n",
       " '[북러 회담] 통일부 \"푸틴, 소련의 6·25 참전 첫 공식인정\"',\n",
       " '與당권, 나경원·원희룡·한동훈 ‘3파전’ 구도…결선투표 변수로',\n",
       " \"정동만 1호 법안 '고준위 특별법' 발의\",\n",
       " '대통령 부인에 명품백·시계 선물 되나? 권익위 \"직무 관련 없으면 가능\"…스스로 권위 깎는 권익위',\n",
       " '러북 비판 없는 이재명에 김기현 \"김정은 수석대변인 같아\"',\n",
       " '“많이·빠르게 감소”',\n",
       " '한동훈 \"23일 출마 선언\"...\\'민주당의 아버지\\' 여진',\n",
       " '윤석열 ‘개인폰’ 통화 기록, 왜 자꾸 나올까?',\n",
       " '한동훈 23일 당대표 출마 선언…이재명 연임 도전 수순',\n",
       " '野, 채상병특검 청문회 불출석 증인 ‘무더기 고발’ 예고…“신원식·김계환만 사유서 제출”',\n",
       " '나경원 \"근본 깨는 이재명, 대한민국 비명횡사..무조건 막아야, 곧 결심\"',\n",
       " '박원석 \"원희룡 출마? 용산 참전신호\"...김근식 \"결선투표 노린 듯\"',\n",
       " '나경원 “당 대표는 대통령과의 갈등이 겉으로 드러나면 안 돼…차별화해 대권 가겠다는 건 미래 없어”',\n",
       " '최진녕 변호사 / 서용주 전 더불어민주당 상근부대변인 - 원 구성 놓고 여야 입장 차…대치 정국 심화',\n",
       " '뉴스홈',\n",
       " '사회',\n",
       " '정치',\n",
       " '경제',\n",
       " '국제',\n",
       " '문화',\n",
       " 'IT',\n",
       " '포토',\n",
       " '제휴 언론사',\n",
       " '배열이력',\n",
       " '전체뉴스',\n",
       " '연재',\n",
       " '팩트체크']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "n_list = []\n",
    "url = 'https://news.daum.net/politics#a1asdasd'\n",
    "headers = {}\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "target = soup.select('.link_txt')\n",
    "\n",
    "for idx, i in enumerate(target):\n",
    "    n_list.append(i.text)\n",
    "    if idx ==150:\n",
    "        break\n",
    "n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_url.format(query): URL 문자열 내의 특정 부분을 query 변수의 값으로 대체\n",
    "- news_url 변수에는 포맷 문자열 'https://news.example.com/search?query={}'가 저장\n",
    "- news_url.format(query)는 포맷 문자열의 {} 부분을 query 변수의 값으로 대체\n",
    "- 결과적으로, 포맷팅된 URL은 'https://news.example.com/search?query=politics'가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "query = input('검색 키워드를 입력하세요 : ')\n",
    "query = query.replace(' ' , '+')\n",
    "print(query)\n",
    "      \n",
    "nuews_url='https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "# format(query) 메서드는 URL템플릿의 {}부분을 querly로 대체\n",
    "req = requests.get(nuews_url.format(query))\n",
    "\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.select('.news_tit')\n",
    "for link in links:\n",
    "    title = link.text\n",
    "    url = link.attrs['href']\n",
    "    print(title, '\\n', url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3_0620. 앞에서 출력한 기사 리스트를 pandas 데이터프레임으로 변환 후 csv 파일로 저장 후 다시 불러오세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>핵잠·ICBM 기술 이전 길 텄나…北-러 ‘방위능력 공동조치’ 명시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>여·야, 원구성 협상 또 합의점 못 찾아… 입장차만 확인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>농해수위, 야당 주도 '양곡관리법·농안법' 상정…여당 불참</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>우상호 \"저도 '왕수박'으로 몰려…당에 '나 같은 역할' 안 보여\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>부산시의회, 부산테크노파크 원장 '2+1년' 이후 연임 집중 질의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>野, 채상병특검 청문회 불출석 증인 ‘무더기 고발’ 예고…“신원식·김계환만 사유서 제출”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>나경원 \"근본 깨는 이재명, 대한민국 비명횡사..무조건 막아야, 곧 결심\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>박원석 \"원희룡 출마? 용산 참전신호\"...김근식 \"결선투표 노린 듯\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>나경원 “당 대표는 대통령과의 갈등이 겉으로 드러나면 안 돼…차별화해 대권 가겠다는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>최진녕 변호사 / 서용주 전 더불어민주당 상근부대변인 - 원 구성 놓고 여야 입장 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            n_title\n",
       "0             0              핵잠·ICBM 기술 이전 길 텄나…北-러 ‘방위능력 공동조치’ 명시\n",
       "1             1                    여·야, 원구성 협상 또 합의점 못 찾아… 입장차만 확인\n",
       "2             2                   농해수위, 야당 주도 '양곡관리법·농안법' 상정…여당 불참\n",
       "3             3              우상호 \"저도 '왕수박'으로 몰려…당에 '나 같은 역할' 안 보여\"\n",
       "4             4               부산시의회, 부산테크노파크 원장 '2+1년' 이후 연임 집중 질의\n",
       "..          ...                                                ...\n",
       "245         245  野, 채상병특검 청문회 불출석 증인 ‘무더기 고발’ 예고…“신원식·김계환만 사유서 제출”\n",
       "246         246          나경원 \"근본 깨는 이재명, 대한민국 비명횡사..무조건 막아야, 곧 결심\"\n",
       "247         247            박원석 \"원희룡 출마? 용산 참전신호\"...김근식 \"결선투표 노린 듯\"\n",
       "248         248  나경원 “당 대표는 대통령과의 갈등이 겉으로 드러나면 안 돼…차별화해 대권 가겠다는...\n",
       "249         249  최진녕 변호사 / 서용주 전 더불어민주당 상근부대변인 - 원 구성 놓고 여야 입장 ...\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.to_csv('test.csv')\n",
    "\n",
    "df1=pd.read_csv('test.csv')\n",
    "\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
